upper_bound = c( 2,  4,  3)
L			= 10000 			# Number of points to compare true function and approximation
# Initialize Chebyshev approximator
cheb = initializeChebyshevApproximator(N_dim, N_degree, N_nodes, lower_bound, upper_bound)
# Calculate Chebyshev regression coefficients to approximate f
cheb = calculateChebyshevCoefficients(f, cheb)
set.seed(124)
Z = matrix(0, nrow = L, ncol = N_dim)
for (k in 1:N_dim)
Z[,k] = runif(L, min = lower_bound[k], max = upper_bound[k])
Z
i = 1
z = calculateChebyshevPolynomials(cheb$N_degree, Z[i,K])
[,k]
Z
TZ = matrix(0, nrow = L, ncol = (cheb$N_degree + 1)^cheb$N_dim)
calculateChebyshevPolynomials(cheb$N_degree, Z[i,K])
K
L = dim(Z)[1]
K = dim(Z)[2]
K
z = calculateChebyshevPolynomials(cheb$N_degree, Z[i,K])
z
calculateChebyshevPolynomials(cheb$N_degree, Z[i,k]) %x% z
calculateChebyshevPolynomials(cheb$N_degree, Z[i,k])
z
calculateChebyshevPolynomials(cheb$N_degree, Z[i,k]) %x% z
x
calculateChebyshevPolynomials(cheb$N_degree, Z[i,k])
z
L = dim(Z)[1]
K = dim(Z)[2]
Z[,k] = 2*(Z[,k]-cheb$lower_bound[k])/(cheb$upper_bound[k] - cheb$lower_bound[k]) - 1
Z
for (k in 1:K)
Z[,k] = 2*(Z[,k]-cheb$lower_bound[k])/(cheb$upper_bound[k] - cheb$lower_bound[k]) - 1
TZ = matrix(0, nrow = L, ncol = (cheb$N_degree + 1)^cheb$N_dim)
i
z = calculateChebyshevPolynomials(cheb$N_degree, Z[i,K])
z
calculateChebyshevPolynomials(cheb$N_degree, Z[i,k])
k
k = K-1
calculateChebyshevPolynomials(cheb$N_degree, Z[i,k])
z
calculateChebyshevPolynomials(cheb$N_degree, Z[i,k]) %x% z
dim(calculateChebyshevPolynomials(cheb$N_degree, Z[i,k]) %x% z)
for (k in (K-1):1) z = calculateChebyshevPolynomials(cheb$N_degree, Z[i,k]) %x% z
z
size(z)
dim(z)
(K-1):1
4:1
#	Chebyshev-Approximation.R
#	-------------------------------------------------------------------------------------
#	GÃ¼nter J. Hitsch, April 2014
#	Uses Chebyshev approximation to approximate some function f
rm(list = ls())
source("Chebyshev-Approximator.R")
# Functions to approximate
# Polynomial
#f = function(X) {
#	X1 = X[,1]
#	X2 = X[,2]
#	X3 = X[,3]
#	return( X1*X3^3 + X2*X3 + X1^2*X2*X3^2  )
#}
# Medium difficult
f = function(X) {
return( X[,1]*exp(5 + X[,2])*X[,3] )
}
# More difficult
#f = function(X) {
#	return( X[,1]^2*cos(X[,2])*exp(X[,3]) )
#}
# Settings
N_dim 		= 3
N_degree	= 5
N_nodes		= 9
lower_bound = c(-5, -2, -3)		# Rectangle bounds
upper_bound = c( 2,  4,  3)
L			= 10000 			# Number of points to compare true function and approximation
# Initialize Chebyshev approximator
cheb = initializeChebyshevApproximator(N_dim, N_degree, N_nodes, lower_bound, upper_bound)
# Calculate Chebyshev regression coefficients to approximate f
cheb = calculateChebyshevCoefficients(f, cheb)
# Matrix for comparison
#set.seed(124)
Z = matrix(0, nrow = L, ncol = N_dim)
for (k in 1:N_dim)
Z[,k] = runif(L, min = lower_bound[k], max = upper_bound[k])
# True and predicted values
y		= f(Z)
y_hat	= evaluateChebyshev(Z, cheb)
# Mean and max absolute difference between true and predicted values
diff		= abs(y - y_hat)
mean_diff	= mean(diff)
max_diff	= max(diff)
#print(cbind(y, y_hat))
print(mean(diff))
print(max(diff))
#	Chebyshev-Approximation.R
#	-------------------------------------------------------------------------------------
#	GÃ¼nter J. Hitsch, April 2014
#	Uses Chebyshev approximation to approximate some function f
rm(list = ls())
source("Chebyshev-Approximator.R")
# Functions to approximate
# Polynomial
#f = function(X) {
#	X1 = X[,1]
#	X2 = X[,2]
#	X3 = X[,3]
#	return( X1*X3^3 + X2*X3 + X1^2*X2*X3^2  )
#}
# Medium difficult
f = function(X) {
return( X[,1]*exp(5 + X[,2])*X[,3] )
}
# More difficult
#f = function(X) {
#	return( X[,1]^2*cos(X[,2])*exp(X[,3]) )
#}
# Settings
N_dim 		= 3
N_degree	= 5
N_nodes		= 9
lower_bound = c(-5, -2, -3)		# Rectangle bounds
upper_bound = c( 2,  4,  3)
L			= 10000 			# Number of points to compare true function and approximation
# Initialize Chebyshev approximator
cheb = initializeChebyshevApproximator(N_dim, N_degree, N_nodes, lower_bound, upper_bound)
# Calculate Chebyshev regression coefficients to approximate f
cheb = calculateChebyshevCoefficients(f, cheb)
# Matrix for comparison
#set.seed(124)
Z = matrix(0, nrow = L, ncol = N_dim)
for (k in 1:N_dim)
Z[,k] = runif(L, min = lower_bound[k], max = upper_bound[k])
# True and predicted values
y		= f(Z)
y_hat	= evaluateChebyshev(Z, cheb)
# Mean and max absolute difference between true and predicted values
diff		= abs(y - y_hat)
mean_diff	= mean(diff)
max_diff	= max(diff)
#print(cbind(y, y_hat))
print(mean(diff))
print(max(diff))
#	Chebyshev-Approximation.R
#	-------------------------------------------------------------------------------------
#	GÃ¼nter J. Hitsch, April 2014
#	Uses Chebyshev approximation to approximate some function f
rm(list = ls())
source("Chebyshev-Approximator.R")
# Functions to approximate
# Polynomial
#f = function(X) {
#	X1 = X[,1]
#	X2 = X[,2]
#	X3 = X[,3]
#	return( X1*X3^3 + X2*X3 + X1^2*X2*X3^2  )
#}
# Medium difficult
f = function(X) {
return( X[,1]*exp(5 + X[,2])*X[,3] )
}
# More difficult
#f = function(X) {
#	return( X[,1]^2*cos(X[,2])*exp(X[,3]) )
#}
# Settings
N_dim 		= 3
N_degree	= 5
N_nodes		= 9
lower_bound = c(-5, -2, -3)		# Rectangle bounds
upper_bound = c( 2,  4,  3)
L			= 10000 			# Number of points to compare true function and approximation
# Initialize Chebyshev approximator
cheb = initializeChebyshevApproximator(N_dim, N_degree, N_nodes, lower_bound, upper_bound)
# Calculate Chebyshev regression coefficients to approximate f
cheb = calculateChebyshevCoefficients(f, cheb)
# Matrix for comparison
#set.seed(124)
Z = matrix(0, nrow = L, ncol = N_dim)
for (k in 1:N_dim)
Z[,k] = runif(L, min = lower_bound[k], max = upper_bound[k])
# True and predicted values
y		= f(Z)
y_hat	= evaluateChebyshev(Z, cheb)
# Mean and max absolute difference between true and predicted values
diff		= abs(y - y_hat)
mean_diff	= mean(diff)
max_diff	= max(diff)
#print(cbind(y, y_hat))
print(mean(diff))
print(max(diff))
#	Chebyshev-Approximation.R
#	-------------------------------------------------------------------------------------
#	GÃ¼nter J. Hitsch, April 2014
#	Uses Chebyshev approximation to approximate some function f
rm(list = ls())
source("Chebyshev-Approximator.R")
# Functions to approximate
# Polynomial
#f = function(X) {
#	X1 = X[,1]
#	X2 = X[,2]
#	X3 = X[,3]
#	return( X1*X3^3 + X2*X3 + X1^2*X2*X3^2  )
#}
# Medium difficult
f = function(X) {
return( X[,1]*exp(5 + X[,2])*X[,3] )
}
# More difficult
#f = function(X) {
#	return( X[,1]^2*cos(X[,2])*exp(X[,3]) )
#}
# Settings
N_dim 		= 3
N_degree	= 5
N_nodes		= 9
lower_bound = c(-5, -2, -3)		# Rectangle bounds
upper_bound = c( 2,  4,  3)
L			= 10000 			# Number of points to compare true function and approximation
# Initialize Chebyshev approximator
cheb = initializeChebyshevApproximator(N_dim, N_degree, N_nodes, lower_bound, upper_bound)
# Calculate Chebyshev regression coefficients to approximate f
cheb = calculateChebyshevCoefficients(f, cheb)
# Matrix for comparison
#set.seed(124)
Z = matrix(0, nrow = L, ncol = N_dim)
for (k in 1:N_dim)
Z[,k] = runif(L, min = lower_bound[k], max = upper_bound[k])
# True and predicted values
y		= f(Z)
y_hat	= evaluateChebyshev(Z, cheb)
# Mean and max absolute difference between true and predicted values
diff		= abs(y - y_hat)
mean_diff	= mean(diff)
max_diff	= max(diff)
#print(cbind(y, y_hat))
print(mean(diff))
print(max(diff))
#	Chebyshev-Approximation.R
#	-------------------------------------------------------------------------------------
#	GÃ¼nter J. Hitsch, April 2014
#	Uses Chebyshev approximation to approximate some function f
rm(list = ls())
source("Chebyshev-Approximator.R")
# Functions to approximate
# Polynomial
#f = function(X) {
#	X1 = X[,1]
#	X2 = X[,2]
#	X3 = X[,3]
#	return( X1*X3^3 + X2*X3 + X1^2*X2*X3^2  )
#}
# Medium difficult
f = function(X) {
return( X[,1]*log(5 + X[,2])*X[,3] )
}
# More difficult
#f = function(X) {
#	return( X[,1]^2*cos(X[,2])*exp(X[,3]) )
#}
# Settings
N_dim 		= 3
N_degree	= 5
N_nodes		= 9
lower_bound = c(-5, -2, -3)		# Rectangle bounds
upper_bound = c( 2,  4,  3)
L			= 10000 			# Number of points to compare true function and approximation
# Initialize Chebyshev approximator
cheb = initializeChebyshevApproximator(N_dim, N_degree, N_nodes, lower_bound, upper_bound)
# Calculate Chebyshev regression coefficients to approximate f
cheb = calculateChebyshevCoefficients(f, cheb)
# Matrix for comparison
set.seed(124)
Z = matrix(0, nrow = L, ncol = N_dim)
for (k in 1:N_dim)
Z[,k] = runif(L, min = lower_bound[k], max = upper_bound[k])
# True and predicted values
y		= f(Z)
y_hat	= evaluateChebyshev(Z, cheb)
# Mean and max absolute difference between true and predicted values
diff		= abs(y - y_hat)
mean_diff	= mean(diff)
max_diff	= max(diff)
#print(cbind(y, y_hat))
print(mean(diff))
print(max(diff))
f = function(X) {
return( X[,1]^2*cos(X[,2])*exp(X[,3]) )
}
# Settings
N_dim 		= 3
N_degree	= 5
N_nodes		= 9
lower_bound = c(-5, -2, -3)		# Rectangle bounds
upper_bound = c( 2,  4,  3)
L			= 10000 			# Number of points to compare true function and approximation
# Initialize Chebyshev approximator
cheb = initializeChebyshevApproximator(N_dim, N_degree, N_nodes, lower_bound, upper_bound)
# Calculate Chebyshev regression coefficients to approximate f
cheb = calculateChebyshevCoefficients(f, cheb)
# Matrix for comparison
set.seed(124)
Z = matrix(0, nrow = L, ncol = N_dim)
for (k in 1:N_dim)
Z[,k] = runif(L, min = lower_bound[k], max = upper_bound[k])
# True and predicted values
y		= f(Z)
y_hat	= evaluateChebyshev(Z, cheb)
# Mean and max absolute difference between true and predicted values
diff		= abs(y - y_hat)
mean_diff	= mean(diff)
max_diff	= max(diff)
#print(cbind(y, y_hat))
print(mean(diff))
print(max(diff))
#	Integration.R
#	-------------------------------------------------------------------------------------
#	GÃ¼nter J. Hitsch, May 2014
#	Uses Gauss-Hermite quadrature to calculate an integral and compares with Monte Carlo
#	integration method
rm(list = ls())
source("Gauss-Hermite-Quadrature.R")
# Settings ------------------------------------------------------------------------------
N_dim 		= 3						# Dimensionality = number of function arguments
N_nodes		= 9						# No. of quadrature nodes
L			= 10000000 				# Number of simulation draws
mu = c(0.5, -1.0, 0.0)				# Multivariate normal distribution: mean
# Covariance matrix: creative positive definite matrix (a valid covariance matrix) from A
A	= matrix( c(  1.0,  0.2, -0.5,
0.2,  1.6,  0.9,
-0.5,  0.9,  0.2),  nrow=3, ncol=3, byrow=TRUE)
Cov = t(A) %*% A					# Covariance matrix of multivariate normal distribution
chol_Cov = chol(Cov)				# Cholesky decomposition
chol_Cov
N_x
x
result = gauher_mdim(K)
N_nodes
N_dim
dim = N_dim
K	= N_nodes
## Sub-function gauher_mdim
## (Exact copy of gauher)
## Returns result as a list of x and w
gauher_mdim = function(n) {
EPS = 3.0e-14
PIM4 = 0.7511255444649425
MAXIT = 10
x = matrix(0,nrow=n,ncol=1)
w = matrix(0,nrow=n,ncol=1)
m=(n+1)/2
for (i in 1:m) {
if (i == 1)
z=sqrt((2*n+1))-1.85575*(2*n+1)^(-0.16667)
else if (i == 2)
z = z - 1.14*(n^0.426)/z
else if (i == 3)
z=1.86*z-0.86*x[1]
else if (i == 4)
z=1.91*z-0.91*x[2]
else
z=2.0*z-x[i-2]
for (its in 1:MAXIT) {
p1=PIM4
p2=0.0
for (j in 1:n) {
p3=p2
p2=p1
p1=z*sqrt(2.0/j)*p2-sqrt(((j-1))/j)*p3
}
pp=sqrt(2*n)*p2
z1=z
z=z1-p1/pp
if (abs(z-z1) <= EPS) break
}
x[i] = z
x[n+1-i] = -z
w[i]=2.0/(pp*pp)
w[n+1-i]=w[i]
}
result = list()
result$x = x
result$w = w
return(result)
}
## Sub-function expand_mdim
expand_mdim = function(x,y) {
N_x = nrow(x)
N_y = nrow(y)
z = c()
for (i in 1:N_x) {
D = matrix(x[i,], nrow=N_y, ncol=ncol(x), byrow=T)
E = cbind(D, y)
z = rbind(z, E)
}
return(z)
}
result = gauher_mdim(K)
X = result$x
W = result$w
X
W
for (i in 2:dim) {
X = expand_mdim(result$x,X)
W = expand_mdim(result$w,W)
}
X
result$
result$x
result$x
X
N_x
N_x = nrow(x)
X = result$x
W = result$w
X
W
result$x
X
y = W
N_x = nrow(x)
N_y = nrow(y)
N_y
N_x
N_x
N_x = nrow(x)
x = X
N_x = nrow(x)
N_x
N_y
i = 1
x[i,]
matrix(x[i,], nrow=N_y, ncol=ncol(x), byrow=T)
matrix(x[i,], nrow=N_y, ncol=ncol(x), byrow=T)
cbind(D, y)
D
D = matrix(x[i,], nrow=N_y, ncol=ncol(x), byrow=T)
cbind(D, y)
## Create a list of nodes and weights
X = result$x
W = result$w
for (i in 2:dim) {
X = expand_mdim(result$x,X)
W = expand_mdim(result$w,W)
}
W
size(W)
dim(W)
apply(W, 1, cumprod)
size(apply(W, 1, cumprod))
dim(apply(W, 1, cumprod))
dim(W)
apply(W, 1, cumprod)
t(apply(W, 1, cumprod))
W
apply(W, 1, cumprod)
t(apply(W, 1, cumprod))
t(apply(W, 1, cumprod))[end,]
t(apply(W, 1, cumprod))[-1,]
weight = t(apply(W, 1, cumprod))
View(weight)
matrix(weight[,dim], ncol=1)
dim
weight[,dim]
matrix(weight[,dim], ncol=1)
#	Integration.R
#	-------------------------------------------------------------------------------------
#	GÃ¼nter J. Hitsch, May 2014
#	Uses Gauss-Hermite quadrature to calculate an integral and compares with Monte Carlo
#	integration method
rm(list = ls())
source("Gauss-Hermite-Quadrature.R")
# Settings ------------------------------------------------------------------------------
N_dim 		= 3						# Dimensionality = number of function arguments
N_nodes		= 9						# No. of quadrature nodes
L			= 10000000 				# Number of simulation draws
mu = c(0.5, -1.0, 0.0)				# Multivariate normal distribution: mean
# Covariance matrix: creative positive definite matrix (a valid covariance matrix) from A
A	= matrix( c(  1.0,  0.2, -0.5,
0.2,  1.6,  0.9,
-0.5,  0.9,  0.2),  nrow=3, ncol=3, byrow=TRUE)
Cov = t(A) %*% A					# Covariance matrix of multivariate normal distribution
chol_Cov = chol(Cov)				# Cholesky decomposition
# Functions to integrate ----------------------------------------------------------------
f = function(X) {
x1 = X[,1]
x2 = X[,2]
x3 = X[,3]
return(sin(2*x1)*x2^2 + x3)
}
# Initialize quadrature nodes and weights
gauss_hermite = GaussHermite(N_dim, N_nodes)
gauss_hermite
gauss_hermite$X
gauss_hermite$weight
gauss_hermite$X
gauss_hermite$X %*% chol_Cov
%*%
size(v)
size(chol_Cov)
gauss_hermite$X
gauss_hermite$X %*% chol_Cov
dim(gauss_hermite$X %*% chol_Cov)
dim(v)
dim(gauss_hermite$X)
